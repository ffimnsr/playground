{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f64a236-060b-4648-89fb-8ad86addde67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "!pip3 install -q --upgrade pip\n",
    "!pip3 install -q google-cloud-aiplatform\n",
    "!pip3 install -q langchain\n",
    "!pip3 install -q langchain-community\n",
    "!pip3 install -q lxml\n",
    "!pip3 install -q requests\n",
    "!pip3 install -q beautifulsoup4\n",
    "!pip3 install -q unstructured\n",
    "!pip3 install -q langchain-google-genai\n",
    "!pip3 install -q google-generativeai\n",
    "!pip3 install -q tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b371c287-5c4d-4f6d-9483-787ddb7a79c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart the kernel\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ac7c23-8831-4594-a9aa-a1aade9da542",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f13fb6-6a06-4dcb-9812-4643cb1f9cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import textwrap\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d754a634-726f-4e04-bbec-ff241cfbdb74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# source API key from GCP project and configure genai client\n",
    "import os\n",
    "import pathlib\n",
    "import textwrap\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "key_name = !gcloud services api-keys list --filter=\"gemini-api-key\" --format=\"value(name)\"\n",
    "key_name = key_name[0]\n",
    "\n",
    "api_key = !gcloud services api-keys get-key-string $key_name --location=\"us-central1\" --format=\"value(keyString)\"\n",
    "api_key = api_key[0]\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7db35058-5ee4-4eda-94d1-1c8fa96235fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your project ID is: qwiklabs-gcp-04-6e6a03811e15\n"
     ]
    }
   ],
   "source": [
    "# Define project information\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "PROJECT_ID = subprocess.check_output([\"gcloud\", \"config\", \"get-value\", \"project\"], text=True).strip()\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "print(f\"Your project ID is: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9781195e-7ab6-455b-9397-e166352bfb09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set environment vars\n",
    "BUCKET = f\"gs://{PROJECT_ID}/embeddings\"\n",
    "DIMENSIONS=768\n",
    "DISPLAY_NAME='vertex_docs_qa'\n",
    "ENDPOINT=f\"{REGION}-aiplatform.googleapis.com\"\n",
    "TEXT_GENERATION_MODEL='gemini-pro'\n",
    "SITEMAP='https://docs.anthropic.com/sitemap.xml'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c15d201b-404b-4fbb-8ff2-7e45096ab2d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f243a82-fb52-4874-a921-978a5a2769c4",
   "metadata": {},
   "source": [
    "# Task 1: Create Documents from Vertex AI Cloud Documentation Site\n",
    "\n",
    "## Load and parse sitemap.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd6c90bd-1d13-4b5b-baaa-9d17b6097b19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse the xml of sitemap and get URLs of doc site\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_sitemap(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"xml\")\n",
    "    urls = [element.text for element in soup.find_all(\"loc\")]\n",
    "    return urls\n",
    "\n",
    "sites = parse_sitemap(SITEMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a375b356-263c-481a-89eb-19eeacbf2088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use this to filter out docs that don't have a corresponding reference page\n",
    "sites_filtered = [url for url in sites if '/en/docs' in url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c956153-9c15-42b2-83fc-6c30c13d21c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sites_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc151f1f-0416-4421-909d-b792750220d2",
   "metadata": {},
   "source": [
    "## Load documentation pages using the LangChain UnstructuredURLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82632343-60bd-4c3c-a39d-ae728f75d1b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This step will take a few minutes to complete\n",
    "# you will see download messages below the cell after execution\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "loader = UnstructuredURLLoader(urls=sites_filtered)\n",
    "documents = loader.load();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d537a9e4-85e1-44c8-9c91-fd32bc527566",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> Anthropic home page\n",
       "> \n",
       "> Use cases\n",
       "> \n",
       "> Classification\n",
       "> \n",
       "> User GuidesAPI ReferencePrompt LibraryRelease NotesBuild with Claude Contest\n",
       "> \n",
       "> Developer Console\n",
       "> \n",
       "> Developer Discord\n",
       "> \n",
       "> Support\n",
       "> \n",
       "> Get started\n",
       "> \n",
       "> Overview\n",
       "> \n",
       "> Quickstart\n",
       "> \n",
       "> Intro to Claude\n",
       "> \n",
       "> Learn about Claude\n",
       "> \n",
       "> Use cases\n",
       "> \n",
       "> Overview\n",
       "> \n",
       "> Classification\n",
       "> \n",
       "> Content moderation\n",
       "> \n",
       "> Ticket routing\n",
       "> \n",
       "> Models\n",
       "> \n",
       "> Security and compliance\n",
       "> \n",
       "> Build with Claude\n",
       "> \n",
       "> Define success criteria\n",
       "> \n",
       "> Develop test cases\n",
       "> \n",
       "> Prompt engineering\n",
       "> \n",
       "> Text generation\n",
       "> \n",
       "> Embeddings\n",
       "> \n",
       "> Google Sheets add-on\n",
       "> \n",
       "> Vision\n",
       "> \n",
       "> Tool use (function calling)\n",
       "> \n",
       "> Test and evaluate\n",
       "> \n",
       "> Strengthen guardrails\n",
       "> \n",
       "> Using the Evaluation Tool\n",
       "> \n",
       "> Resources\n",
       "> \n",
       "> Glossary\n",
       "> \n",
       "> System status\n",
       "> \n",
       "> Claude 3 model card\n",
       "> \n",
       "> Anthropic Cookbook\n",
       "> \n",
       "> Anthropic Courses\n",
       "> \n",
       "> Use cases\n",
       "> \n",
       "> Classification\n",
       "> \n",
       "> Claude excels at processing, understanding, and recognizing patterns in text, images, and data. These capabilities make Claude especially powerful for classification tasks.\n",
       "> \n",
       "> This guide walks through the process of determining the best approach for building a classifier with Claude and the essentials of end-to-end deployment for a Claude classifier, from use case exploration to back-end integration.\n",
       "> \n",
       "> Visit our classification cookbooks to see example classification implementations using Claude.\n",
       "> \n",
       "> When to use Claude for classification\n",
       "> \n",
       "> When should you consider using an LLM instead of a traditional ML approach for your classification tasks? Here are some key indicators:\n",
       "> \n",
       "> Rule-based classes: Use Claude when classes are defined by conditions rather than examples, as it can understand underlying rules.\n",
       "> \n",
       "> Evolving classes: Claude adapts well to new or changing domains with emerging classes and shifting boundaries.\n",
       "> \n",
       "> Unstructured inputs: Claude can handle large volumes of unstructured text inputs of varying lengths.\n",
       "> \n",
       "> Limited labeled examples: With few-shot learning capabilities, Claude learns accurately from limited labeled training data.\n",
       "> \n",
       "> Reasoning Requirements: Claude excels at classification tasks requiring semantic understanding, context, and higher-level reasoning.\n",
       "> \n",
       "> Establish your classification use case\n",
       "> \n",
       "> Below is a non-exhaustive list of common classification use cases where Claude excels by industry.\n",
       "> \n",
       "> Content moderation: automatically identify and flag inappropriate, offensive, or harmful content in user-generated text, images, or videos.\n",
       "> \n",
       "> Bug prioritization: calassify software bug reports based on their severity, impact, or complexity to prioritize development efforts and allocate resources effectively.\n",
       "> \n",
       "> Intent analysis: determine what the user wants to achieve or what action they want the system to perform based on their text inputs.\n",
       "> \n",
       "> Support ticket routing: analyze customer interactions, such as call center transcripts or support tickets, to route issues to the appropriate teams, prioritize critical cases, and identify recurring problems for proactive resolution.\n",
       "> \n",
       "> Patient triaging: classify customer intake conversations and data according to the urgency, topic, or required expertise for efficient triaging.\n",
       "> \n",
       "> Clinical trial screening: analyze patient data and medical records to identify and categorize eligible participants based on specified inclusion and exclusion criteria.\n",
       "> \n",
       "> Fraud detection: identify suspicious patterns or anomalies in financial transactions, insurance claims, or user behavior to prevent and mitigate fraudulent activities.\n",
       "> \n",
       "> Credit risk assessment: classify loan applicants based on their creditworthiness into risk categories to automate credit decisions and optimize lending processes.\n",
       "> \n",
       "> Legal document categorization: classify legal documents, such as pleadings, motions, briefs, or memoranda, based on their document type, purpose, or relevance to specific cases or clients.\n",
       "> \n",
       "> Implement Claude for classification\n",
       "> \n",
       "> The three key model decision factors are: intelligence, latency, and price.\n",
       "> \n",
       "> For classification, a smaller model like Claude 3 Haiku is typically ideal due to its speed and efficiency. Though, for classification tasks where specialized knowledge or complex reasoning is required, Sonnet or Opus may be a better choice. Learn more about how Opus, Sonnet, and Haiku compare here.\n",
       "> \n",
       "> Use evaluations to gauge whether a Claude model is performing well enough to launch into production.\n",
       "> \n",
       "> 1. Build a strong input prompt\n",
       "> \n",
       "> While Claude offers high-level baseline performance out of the box, a strong input prompt helps get the best results.\n",
       "> \n",
       "> For a generic classifier that you can adapt to your specific use case, copy the starter prompt below:\n",
       "> \n",
       "> You will be building a text classifier that can automatically categorize text into a set of predefined categories. \n",
       "> Here are the categories the classifier will use:\n",
       "> \n",
       "> <categories>\n",
       "> {{CATEGORIES}}\n",
       "> </categories>\n",
       "> \n",
       "> To help you understand how to classify text into these categories, here are some example texts that have already been labeled with their correct category:\n",
       "> \n",
       "> <examples>\n",
       "> {{EXAMPLES}}\n",
       "> </examples>\n",
       "> \n",
       "> Please carefully study these examples to identify the key features and characteristics that define each category. Write out your analysis of each category inside <category_analysis> tags, explaining the main topics, themes, writing styles, etc. that seem to be associated with each one.\n",
       "> \n",
       "> Once you feel you have a good grasp of the categories, your task is to build a classifier that can take in new, unlabeled texts and output a prediction of which category it most likely belongs to.\n",
       "> \n",
       "> Before giving your final classification, show your step-by-step process and reasoning inside <classification_process> tags. Weigh the evidence for each potential category.\n",
       "> \n",
       "> Then output your final <classification> for which category you think the example text belongs to.\n",
       "> \n",
       "> The goal is to build a classifier that can accurately categorize new texts into the most appropriate category, as defined by the examples.\n",
       "> \n",
       "> We also provide a wide range of prompts to get you started in our prompt library, including prompts for a number of classification use cases, including:\n",
       "> \n",
       "> Sentiment Analysis\n",
       "> \n",
       "> Detect the tone and sentiment behind tweets. Understand user emotions, opinions, and reactions in real-time.\n",
       "> \n",
       "> Customer Review Classification\n",
       "> \n",
       "> Categorize feedback into pre-specified tags. Streamline product insights and customer service responses.\n",
       "> \n",
       "> 2. Develop your test cases\n",
       "> \n",
       "> To run your classification evaluation, you will need test cases to run it on. Take a look at our guide to developing test cases.\n",
       "> \n",
       "> 3. Run your eval\n",
       "> \n",
       "> Evaluation metrics\n",
       "> \n",
       "> Some success metrics to consider evaluating Claudeâ€™s performance on a classification task include:\n",
       "> \n",
       "> Criteria Description Accuracy The modelâ€™s output exactly matches the golden answer or correctly classifies the input according to the taskâ€™s requirements. This is typically calculated as (Number of correct predictions) / (Overall number of predictions). F1 Score The modelâ€™s output optimally balances precision and recall. Consistency The modelâ€™s output is consistent with its predictions for similar inputs or follows a logical pattern. Structure The modelâ€™s output follows the expected format or structure, making it easy to parse and interpret. For example, many classifiers are expected to output JSON format. Speed The model provides a response within the acceptable time limit or latency threshold for the task. Bias and Fairness If classifying data about people, is it important that the model does not demonstrate any biases based on gender, ethnicity, or other characteristics that would lead to its misclassification.\n",
       "> \n",
       "> Deploy your classifier\n",
       "> \n",
       "> To see code examples of how to use Claude for classification, check out the Classification Guide in the Anthropic Cookbook.\n",
       "> \n",
       "> OverviewContent moderation\n",
       "> \n",
       "> xlinkedin\n",
       "> \n",
       "> On this page\n",
       "> \n",
       "> When to use Claude for classification\n",
       "> \n",
       "> Establish your classification use case\n",
       "> \n",
       "> Implement Claude for classification\n",
       "> \n",
       "> 1. Build a strong input prompt\n",
       "> \n",
       "> 2. Develop your test cases\n",
       "> \n",
       "> 3. Run your eval\n",
       "> \n",
       "> Evaluation metrics\n",
       "> \n",
       "> Deploy your classifier\n",
       "> \n",
       "> Source: https://docs.anthropic.com/en/docs/about-claude/use-cases/classification"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_markdown(documents[1].page_content + \"\\n\\nSource: \" + documents[1].metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cacb3e49-cd62-4043-9440-5013eb1797b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297f00b5-4022-467e-a001-008b8a53768c",
   "metadata": {},
   "source": [
    "## Create Document chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eccee30-c90b-4243-a445-53950364b3c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number documents 33\n",
      "Number chunks 197\n"
     ]
    }
   ],
   "source": [
    "# recursively loop through the text and create document chunks for embedding\n",
    "import warnings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    #separator = \"\\n\",\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap  = 100)\n",
    "\n",
    "document_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Number documents {len(documents)}\")\n",
    "print(f\"Number chunks {len(document_chunks)}\")\n",
    "\n",
    "document_chunks=[f\"content: {chunk.page_content}, source: {chunk.metadata['source']}\" for chunk in document_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d7387e-11df-4d72-9fb0-dda3acc098d8",
   "metadata": {},
   "source": [
    "# Task 2: Generate embeddings from Document chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b57362bd-7e38-445c-ada9-20e57132bbc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a documents directory\n",
    "!rm -rf ./documents\n",
    "!mkdir ./documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bc6e60a-aa97-4553-a50d-58649e84df60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content: Anthropic home page\\n\\nLearn about Cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content: Model comparison\\n\\nHere is a visuali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content: Output quality: When migrating from p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>content: Claude 2.1 Claude 2 Claude Instant 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>content: Ticket routingSecurity and compliance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>content: Note: When the response reaches max_t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>content: Anthropic home page\\n\\nStrengthen gua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>content: Notice that this system prompt is sti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>content: Anthropic home page\\n\\nGet started\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>content: Claude can assist with many tasks tha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0    content: Anthropic home page\\n\\nLearn about Cl...\n",
       "1    content: Model comparison\\n\\nHere is a visuali...\n",
       "2    content: Output quality: When migrating from p...\n",
       "3    content: Claude 2.1 Claude 2 Claude Instant 1....\n",
       "4    content: Ticket routingSecurity and compliance...\n",
       "..                                                 ...\n",
       "192  content: Note: When the response reaches max_t...\n",
       "193  content: Anthropic home page\\n\\nStrengthen gua...\n",
       "194  content: Notice that this system prompt is sti...\n",
       "195  content: Anthropic home page\\n\\nGet started\\n\\...\n",
       "196  content: Claude can assist with many tasks tha...\n",
       "\n",
       "[197 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the document chunks in a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(document_chunks, columns =['text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ba0a5ff-26a5-4023-b833-fb9f9b418a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197/197 [00:27<00:00,  7.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to generate the embeddings files you will later upload to Cloud Storage\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "index_embeddings = []\n",
    "model = \"models/embedding-001\"\n",
    "\n",
    "for index, doc in tqdm(df.iterrows(), total=len(df), position=0):\n",
    "\n",
    "    response = genai.embed_content(model=model, content=doc['text'], task_type=\"retrieval_query\")\n",
    "\n",
    "    doc_id=f\"{index}.txt\"\n",
    "    embedding_dict = {\n",
    "        \"id\": doc_id,\n",
    "        \"embedding\": response[\"embedding\"],\n",
    "    }\n",
    "    index_embeddings.append(json.dumps(embedding_dict) + \"\\n\")\n",
    "    \n",
    "    with open(f\"documents/{doc_id}\", \"w\") as document:\n",
    "          document.write(doc['text'])\n",
    "    \n",
    "with open(\"embeddings.json\", \"w\") as f:\n",
    "    f.writelines(index_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93913e92-4c3f-46cc-9498-cde46fe559f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "source_file = '/home/jupyter/embeddings.json'\n",
    "destination_blob_name = 'embeddings/embeddings.json' # Adjust if needed\n",
    "\n",
    "client = storage.Client(project=PROJECT_ID)\n",
    "bucket = client.bucket(PROJECT_ID)\n",
    "blob = bucket.blob(destination_blob_name)\n",
    "blob.upload_from_filename(source_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa9c1278-41f3-4892-bd7d-a9d6eb7848c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['gsutil', '-q', 'cp', '-r', './documents', 'gs://qwiklabs-gcp-04-6e6a03811e15/documents'], returncode=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload the embedding files to Cloud Storage\n",
    "# This step will take a few minutes to complete\n",
    "import subprocess\n",
    "gsutil_command = f\"gsutil -q cp -r './documents' gs://{PROJECT_ID}/documents\"\n",
    "\n",
    "subprocess.run(['gsutil', '-q', 'cp', '-r', './documents', f'gs://{PROJECT_ID}/documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a056208e-7570-4005-b274-90e62d539fb9",
   "metadata": {},
   "source": [
    "# Task 3. Create a Vertex AI Vector Store index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7566d26d-d5eb-4774-80a3-dc61b570fd93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndex\n",
      "Create MatchingEngineIndex backing LRO: projects/618403388557/locations/us-central1/indexes/2469533902310473728/operations/3937614781338353664\n",
      "MatchingEngineIndex created. Resource name: projects/618403388557/locations/us-central1/indexes/2469533902310473728\n",
      "To use this MatchingEngineIndex in another session:\n",
      "index = aiplatform.MatchingEngineIndex('projects/618403388557/locations/us-central1/indexes/2469533902310473728')\n"
     ]
    }
   ],
   "source": [
    "# Create the Vertex AI Vector Search index\n",
    "# This step will take several minutes to complete\n",
    "# Wait for this cell to complete before proceeding\n",
    "index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "      display_name=\"vertex_docs\",\n",
    "      contents_delta_uri=f\"gs://{PROJECT_ID}/embeddings\",\n",
    "      dimensions=768,\n",
    "      approximate_neighbors_count=150,\n",
    "      distance_measure_type=\"DOT_PRODUCT_DISTANCE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ced8d73-3b98-4e55-89ca-a29bcb086d61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndexEndpoint\n",
      "Create MatchingEngineIndexEndpoint backing LRO: projects/618403388557/locations/us-central1/indexEndpoints/3388690438759120896/operations/822249739104813056\n",
      "MatchingEngineIndexEndpoint created. Resource name: projects/618403388557/locations/us-central1/indexEndpoints/3388690438759120896\n",
      "To use this MatchingEngineIndexEndpoint in another session:\n",
      "index_endpoint = aiplatform.MatchingEngineIndexEndpoint('projects/618403388557/locations/us-central1/indexEndpoints/3388690438759120896')\n"
     ]
    }
   ],
   "source": [
    "index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=\"vertex_docs\",\n",
    "    description=\"Embeddings for the documentation curated from the sitemap.\",\n",
    "    public_endpoint_enabled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d4077-275f-43cf-8c55-993e0880d651",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying index MatchingEngineIndexEndpoint index_endpoint: projects/618403388557/locations/us-central1/indexEndpoints/3388690438759120896\n",
      "Deploy index MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/618403388557/locations/us-central1/indexEndpoints/3388690438759120896/operations/7528109584259481600\n"
     ]
    }
   ],
   "source": [
    "# This step will take up to 20 minutes to complete\n",
    "# You can view the deployment in the Vertex AI console on the \"Vector Search\" tab\n",
    "# Wait for this cell to complete before proceeding\n",
    "index_endpoint = index_endpoint.deploy_index(\n",
    "    index=index, deployed_index_id=\"vertex_index_deployment\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877608b4-9496-48c5-8513-799d6324f3aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INDEX_RESOURCE_NAME=index.resource_name\n",
    "index = aiplatform.MatchingEngineIndex(index_name=INDEX_RESOURCE_NAME)\n",
    "\n",
    "deployed_index = index.deployed_indexes\n",
    "deployed_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a81baa-a4f6-4712-8db2-339b3ca74e35",
   "metadata": {},
   "source": [
    "# Task 4: Search Vector Store, add result as context to a query (without using a LangChain Chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f563bb-4417-4eac-b675-2377e151357d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In the next cells you will query the model directly using the Vertex AI python SDK\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores.matching_engine import MatchingEngine\n",
    "from langchain.agents import Tool\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "def search_vector_store(question):\n",
    "\n",
    "    vector_store = MatchingEngine.from_components(\n",
    "                        index_id=INDEX_RESOURCE_NAME,\n",
    "                        region=REGION,\n",
    "                        embedding=embeddings,\n",
    "                        project_id=PROJECT_ID,\n",
    "                        endpoint_id=deployed_index[0].index_endpoint,\n",
    "                        gcs_bucket_name=f\"{PROJECT_ID}\")\n",
    "    \n",
    "    relevant_documentation=vector_store.similarity_search(question, k=8)\n",
    "    context = \"\\n\".join([doc.page_content for doc in relevant_documentation])[:10000]\n",
    "    return str(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42442602-f577-4cb2-b4a9-e26fa2425cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "import warnings\n",
    "\n",
    "# filter warnings for unused libs\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def ask_question(question):\n",
    "    context = search_vector_store(question)\n",
    "\n",
    "    prompt=f\"\"\"\n",
    "        Follow exactly those 3 steps:\n",
    "        1. Read the context below and aggregrate this data\n",
    "        Context : {context}\n",
    "        2. Answer the question using only this context\n",
    "        3. Show the source for your answers\n",
    "        User Question: {question}\n",
    "\n",
    "\n",
    "        If you don't have any context and are unsure of the answer, reply that you don't know about this topic.\n",
    "        \"\"\"\n",
    "\n",
    "    model = GenerativeModel(\"gemini-pro\")\n",
    "    response = model.generate_content(prompt)\n",
    "\n",
    "    return to_markdown(f\"Question: \\n{question} \\n\\n Response: \\n {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61376c0e-0447-4527-b212-e65f6b128fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ask_question(\"How do I reduce prompt leaks?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edef10e5-28ab-49f1-bc35-a783ef03e572",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ask_question(\"What use cases and capabilities does Anthropic support?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730748e8-587d-4a07-b707-9f945e6ec96a",
   "metadata": {},
   "source": [
    "# Task 5: Create Retrieval Augmentation Generation application using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf0e958-9d7b-44f0-8893-a508250fa836",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To answer questions and chain together the prompt, vector search, returned context and model input use a LangChain \"Chain\"\n",
    "# In this case you will use the RetrievalQA chain which is commonly used for Question/Answering applications\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# initialize model using chat\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.0, convert_system_message_to_human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2d89b2-7cab-4b72-ab2c-c0c931c777ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "    Follow exactly those 3 steps:\n",
    "    1. Read the context below and aggregrate this data\n",
    "    Context : {context}\n",
    "    \n",
    "    2. Answer the question using only this context\n",
    "    3. Show the source for your answers\n",
    "    User Question: {question}\n",
    "\n",
    "    If you don't have any context and are unsure of the answer, reply that you don't know about this topic.\n",
    "    \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\",  \"question\"], template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4576213f-c4f0-4603-9ffb-5f4692f425e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores.matching_engine import MatchingEngine\n",
    "\n",
    "vector_store = MatchingEngine.from_components(\n",
    "    index_id=INDEX_RESOURCE_NAME,\n",
    "    region=REGION,\n",
    "    embedding=embeddings,\n",
    "    project_id=PROJECT_ID,\n",
    "    endpoint_id=deployed_index[0].index_endpoint,\n",
    "    gcs_bucket_name=f\"{PROJECT_ID}\"\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type='similarity',\n",
    "    search_kwargs={'k': 1}\n",
    ")\n",
    "\n",
    "# Test the retriever with a simple search performed above\n",
    "to_markdown(retriever.get_relevant_documents(\"How do I get started with Anthropic?\")[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caa14bf-0cab-4955-8fb2-d44682c52756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756d95c-fe7f-4eaa-b1c7-0fa6f3fb5187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ask_question(question: str):\n",
    "    response = qa({\"query\": question})\n",
    "\n",
    "    # since k is set to 1 only return the first source retrieved\n",
    "    source = response['source_documents']\n",
    "    \n",
    "    return to_markdown(f\"Response: \\n\\n {response['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b220d283-bf02-4a4b-9975-dc3d0a4c2b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note: You will see a library warning when running this step\n",
    "ask_question(\"How do I get started with Anthropic?\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-16.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-16:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
