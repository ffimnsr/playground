{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_PfeY3REycp",
        "outputId": "1f24b12b-de23-4925-965c-bb1d3c05d11a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.3/194.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install langchain dependencies\n",
        "!pip install -qU langchain-core\n",
        "!pip install -qU azure-ai-inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart kernel after installs so that your environment can access the new\n",
        "# packages.\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI3Oe0wxFWrz",
        "outputId": "453ed4aa-b312-4328-8394-c61070fca00d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Dict, Iterator, List, Mapping, Optional\n",
        "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain_core.language_models.llms import LLM\n",
        "from langchain_core.outputs import GenerationChunk\n",
        "\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage\n",
        "from azure.ai.inference.models import UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "os.environ[\"GITHUB_TOKEN\"] = userdata.get(\"GITHUB_TOKEN\")\n",
        "\n",
        "\n",
        "class AzureLC(LLM):\n",
        "\n",
        "  client: ChatCompletionsClient = None\n",
        "  model: str = None\n",
        "  endpoint: str = \"https://models.inference.ai.azure.com\"\n",
        "  max_tokens: int = 4096\n",
        "  temperature: float = 0.7\n",
        "  top_p: float = 1\n",
        "\n",
        "  def __init__(self, model_name: str, max_tokens: int, **kwargs: Any):\n",
        "    super(AzureLC, self).__init__()\n",
        "    azure_creds = AzureKeyCredential(os.environ[\"GITHUB_TOKEN\"])\n",
        "    self.client = ChatCompletionsClient(\n",
        "      endpoint=self.endpoint,\n",
        "      credential=azure_creds,\n",
        "      temperature=self.temperature,\n",
        "      max_tokens=self.max_tokens,\n",
        "      top_p=self.top_p,\n",
        "    )\n",
        "    self.model = model_name\n",
        "    self.max_tokens = max_tokens\n",
        "\n",
        "  @property\n",
        "  def _identifying_params(self) -> Mapping[str, Any]:\n",
        "    \"\"\"Get the identifying parameters.\"\"\"\n",
        "    return {\n",
        "      \"model\": self.model,\n",
        "      \"temperature\": self.temperature,\n",
        "      \"top_p\": self.top_p,\n",
        "      \"max_tokens\": self.max_tokens,\n",
        "    }\n",
        "\n",
        "  @property\n",
        "  def _llm_type(self) -> str:\n",
        "    \"\"\"Return type of llm.\"\"\"\n",
        "    return \"AzureLC\"\n",
        "\n",
        "  def _call(\n",
        "      self,\n",
        "      prompt: str,\n",
        "      stop: Optional[List[str]] = None,\n",
        "      run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "      **kwargs: Any,\n",
        "  ) -> str:\n",
        "    \"\"\"Call out to Azure models.\"\"\"\n",
        "    response = self.client.complete(\n",
        "      messages=[\n",
        "        UserMessage(content=prompt),\n",
        "      ],\n",
        "      stop=stop,\n",
        "      model=self.model,\n",
        "      **kwargs,\n",
        "    )\n",
        "\n",
        "    if not response.choices:\n",
        "      return None\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "  def _stream(\n",
        "      self,\n",
        "      prompt: str,\n",
        "      stop: Optional[List[str]] = None,\n",
        "      run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "      **kwargs: Any,\n",
        "  ) -> Iterator[GenerationChunk]:\n",
        "    \"\"\"Stream from a language model.\"\"\"\n",
        "    response = self.client.complete(\n",
        "      messages=[\n",
        "        UserMessage(content=prompt),\n",
        "      ],\n",
        "      stop=stop,\n",
        "      model=self.model,\n",
        "      stream=True,\n",
        "      **kwargs,\n",
        "    )\n",
        "\n",
        "    for update in response:\n",
        "      if not update.choices:\n",
        "        break\n",
        "\n",
        "      chunk = GenerationChunk(text=update.choices[0].delta.content)\n",
        "      if run_manager:\n",
        "        run_manager.on_llm_new_token(chunk.text, chunk=chunk)\n",
        "\n",
        "      yield chunk\n"
      ],
      "metadata": {
        "id": "sA7kMMLvFqHM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = AzureLC(model_name=\"gpt-4o\", max_tokens=4096)\n",
        "print(llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q768bOILMNH5",
        "outputId": "87b76557-94e6-405e-f407-69f34e1c78b3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mAzureLC\u001b[0m\n",
            "Params: {'model': 'gpt-4o', 'temperature': 0.7, 'top_p': 1, 'max_tokens': 4096}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm.invoke(\"This is a foobar thing\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sTOC62VNFMv",
        "outputId": "8a6114ae-9e50-46e5-adc9-4612058b14dc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It seems like you might be referring to something specific, but the context is unclear. \"Foobar\" is often used as a placeholder name in programming and computer science, similar to \"foo\" and \"bar.\" It can be used to represent variables, functions, or other elements when the actual names are either unknown or irrelevant to the discussion.\n",
            "\n",
            "If you have a specific question or need help with a particular topic related to \"foobar,\" please provide more details so I can assist you better!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0QJVIAyxWJnE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}